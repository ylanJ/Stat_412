---
title: "Stat 412"
author: "Yolanda Jin"
date: "24/11/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(glmnet)
library(readr)
library(car)
library(randomForest)
library(dplyr)
library(summarytools)
library(ggplot2)
library(ROCR)
library(caTools)
```

## NA data (Income NA), Minority, Majority Groups

- Added column for NA_indicator

- split groups to minority vs majority group



- Q1: do we want NA to be a separate group? based on EDA, might or might not do so


```{r}
# Read Credit Scoring Data Training Set
#cs_train <- cs_training
cs_train = read.csv("cs-training.csv")

# If we want to split NA to another group
#cs_train_NA <- cs_train[cs_train$MonthlyIncome==NA,]
#head(cs_train_NA)



## 0. NA Monthly Income

# Add col to indicate whether monthly Income is NA or not
cs_train$NA_Indicator <- 0  # Set all NA_Indicator to zero
cs_train$NA_Indicator[is.na(cs_train$MonthlyIncome)] <- 1  # Change NA income indexes to 1
head(cs_train[cs_train$NA_Indicator==1,])  # Check the ones indicated as NA


## 1. Separate minority data vs majority data vs NA data total 150k
cs_train_min <- cs_train[cs_train$SeriousDlqin2yrs==1,]  # 10,026 obs
cs_train_maj <- cs_train[cs_train$SeriousDlqin2yrs==0,]  # 139,974 obs



```


```{r}
summary(cs_train)
```
```{r}

hist(cs_train[,2]) #Response Variable
hist(cs_train[,3]) #RevolvingUtilizationOfUnsecuredLines
hist(cs_train[,4]) #age
hist(cs_train[,5]) #NumberOfTime30.59DaysPastDueNotWorse
hist(cs_train[,6]) #DebtRatio
hist(cs_train[,7]) #MonthlyIncome
hist(cs_train[,8]) #NumberOfOpenCreditLinesAndLoans
hist(cs_train[,9]) #NumberOfTimes90DaysLate
hist(cs_train[,10])#NumberRealEstateLoansOrLines
hist(cs_train[,11])#NumberOfTime60.89DaysPastDueNotWorse
hist(cs_train[,12])#NumberOfDependents
hist(cs_train[,13])#NA_Indicator

```

```{r}
cs_train$HighRevolving <- 0
cs_train$HighRevolving[quantile(cs_train[,3],0.95)] <- 1

cs_train$Many_LessThan2MonthsLate <- 0
cs_train$Many_LessThan2MonthsLate[quantile(cs_train[,5],0.95)] <- 1

cs_train$HighDebtRatio <- 0 
cs_train$HighDebtRatio[quantile(cs_train[,6],0.95)] <- 1

cs_train$Many_CurrentLoans <- 0
cs_train$Many_CurrentLoans[quantile(cs_train[,8],0.95)] <- 1

cs_train$Many_AtLeastThreeMonthsLate <- 0
cs_train$Many_AtLeastThreeMonthsLate[quantile(cs_train[,9],0.95)] <- 1

cs_train$Many_HouseLoans <- 0
cs_train$Many_HouseLoans[quantile(cs_train[,10],0.95)] <- 1

cs_train$Many_TwoToThreeMonthsLate <- 0
cs_train$Many_TwoToThreeMonthsLate[quantile(cs_train[,11],0.95)] <- 1

cs_train$NA_Dependents_are_Mean <- cs_train[,12] #Need to Change NA values to Mean before sorting through quantiles.
cs_train$NA_Dependents_are_Mean[is.na(cs_train[,12])] <- mean(cs_train[,12], na.rm = TRUE)
cs_train$Many_Dependents <- 0
cs_train$Many_Dependents[quantile(cs_train$NA_Dependents_are_Mean,0.95)] <- 1

train1 <-cs_train[cs_train[,3]< quantile(cs_train[,3],0.95),]

```

```{r}
head(cs_train)
```

## Split the data to Testing/Training

```{r}

# Remove NA cases otherwise cannot predict
raw_data <- cs_train
cs_train_omit <- na.omit(raw_data)   # 150k -> 120,269 obs

# Sample 60% of data for training Purpose
n = nrow(cs_train_omit)
na.idx = raw_data$X[-cs_train_omit$X]  # indexes of data with NA values that we removed
n.idx = sample(n, n*0.6) # Indexes for test train split

cs_train = cs_train_omit[n.idx,]  # Training data 72,161 obs
cs_test = cs_train_omit[-n.idx,]  # Testing data  48,108 obs.
cs_NA = raw_data[na.idx,]  # data with NA value  29,731 obs
```

```{r}
train2 <- train1[train1[,5]< quantile(train1[,5],0.99),]
hist(train1[,3]) #Do we make transformation for predictors? Not Normal Distribution
```

```{r}

```

```{r}

```



## EDA

- monthly income = 0

- 30k monthly income = NA

- Dependent = NA 4k

- age 0 remove - only 1 point

- age very old

- group by age group etc

- 13 - 101 or older (maybe cut at 100)

- 80 or more

```{r}
#plot(SeriousDlqin2yrs ~ age, data = cs_train_maj)
cs_train_maj_g1 <- cs_train_maj[cs_train_maj$MonthlyIncome<500000,] # less than 500k monthly income
cs_train_maj_g1 <- cs_train_maj[cs_train_maj$MonthlyIncome<20000,] # less than 20k monthly income
plot(MonthlyIncome ~ age, data = cs_train_maj_g1)

##Shelly EDA Code
tapply(train$RevolvingUtilizationOfUnsecuredLines, train$SeriousDlqin2yrs, median) #use this; data very imbalanced

###Next Steps
#***need final data set which excludes outliers for which we will create final model from 
#do box plots for age group and income; fix axes so that box is readable -Shelly
#histogram of groupings like income by age groups, different colors -Yolanda
#sampling of data --> randomly select instead of adding additional data for response variable -Shelly
#ClassDiscovery package and DataExplorer --> provide initial graphs and analyses of data; also initial clustering



#Remove outliers; maybe only keep anything greater than 10
tapply(train$RevolvingUtilizationOfUnsecuredLines, train$SeriousDlqin2yrs, mean) #d
tapply(train$age, train$SeriousDlqin2yrs, median) #median age of 45 for people defaulting
tapply(train$`NumberOfTime30-59DaysPastDueNotWorse`, train$SeriousDlqin2yrs, mean) #2.4 times for those with delinquency incidence
tapply(train$NumberOfTimes90DaysLate, train$SeriousDlqin2yrs, mean) #2.1 times for those with delinquency incidence vs 0.13 for no delinquency
tapply(train$DebtRatio, train$SeriousDlqin2yrs, median) #0.43 for delinquency incidence vs 0.36 for non delinquency
tapply(train$NumIncome, train$SeriousDlqin2yrs, median) #delinquency monthly income of $3.8K vs $4.4K for those without delinquency
tapply(train$NumberOfTimes90DaysLate, train$SeriousDlqin2yrs, mean)
summary(train$NumIncome)




```

## Question/Goal

- Comparing our model performance against paper's model

- Most important factors


## Ensemble Learning

- Lasso Ensemble Algorithm

- Aggregating base learner: Weighted base=learner


## Balancing Data

- Use clustering and pick one sub-group from majority
  - can try Age/Income
  - try Income/debt ratio

- Use bagging algorithm to create more minority data

- Use NA indicator 0 or 1 (maybe use mean/median)


```{r}
# Cluster Grouping Majority Data (Try to cluster data by age/monthly income)
set.seed(1) # for reproducibility

head(cs_train_maj)

# Test with smaller group based on monthly income range
cs_train_maj_g1 <- cs_train_maj[cs_train_maj$NA_Indicator==0,] # Filter out NA monthly income first
cs_train_maj_g1 <- cs_train_maj_g1[cs_train_maj_g1$MonthlyIncome<20000,]  #38035 obs
head(cs_train_maj_g1)

```

```{r}
# Create a dat with the two predictors of interest
dat <- cs_train_maj_g1[,c(4,7)]  # Age and MonthlyIncome
head(dat)

n_maj <- nrow(dat) # get number of rows

# Initial assignments to three groups that will need to update
assignments <- factor(sample(c(1,2,3), n_maj, replace = TRUE))
#plot(dat, col=assignments, xlim = c(0,110), asp=1)
#plot(dat, col=assignments)
```

```{r}
# Boostrapping Minority Data
set.seed(1) # for reproducibility
# set number of minority data to reproduce
n_add <- 1000

n_min <- nrow(cs_train_min)
n_min
index <- sample(n_min, n_add, replace = TRUE)
#plot(density(index),  main="")  # show density curve of the index we randomized
hist(index, breaks = 100)
min(index)
max(index)
length(index)

# We add the additional data for future analysis
cs_train_min_add <- cs_train_min[index,]
head(cs_train_min_add)
```

## Which models to try

- Logistic regression (compare the different link functions)

- look maybe merge Lasso with logistic regression

- RF

## Evaluating Model/Comparing results

- AUC



```{r}
model <- glm(cs_train$SeriousDlqin2yrs ~ cs_train[,3] +cs_train[,4] +cs_train[,5] +cs_train[,6] +cs_train[,7] +cs_train[,8] +cs_train[,9] +cs_train[,10] +cs_train[,11] +cs_train[,12], family = "binomial", data = cs_train) 
#To Change family link use family = quasi(variance = "mu^3", link = "log") change quasi
summary(model)
plot(model, which = 1) #Outliers skew residuals plot
```
```{r}
stepAIC(model)
```
```{r}
model2 <- glm(cs_train$SeriousDlqin2yrs ~ MonthlyIncome + cs_train[,13] + cs_train[,14] + cs_train[,15] + cs_train[,16] + cs_train[,17] + cs_train[,18] + cs_train[,19] + cs_train[,20] + cs_train[,22], family = "binomial", data = cs_train)
model2
model2$rank # equals 7 which is how many variables are not NA
#Certain Dummy Variables are a  Singular Matrix Meaning that some of our variables can be constructed using a linear combination of some of the columns
#Not Sure what to do, but I'll remove these variables in the meantime
```

```{r}
model3 <- glm(cs_train$SeriousDlqin2yrs ~ MonthlyIncome + cs_train[,15] + cs_train[,16] + cs_train[,17] + cs_train[,18] + cs_train[,19], family = "binomial", data = cs_train)
model3
```

```{r}
model4 <- glm(cs_train$SeriousDlqin2yrs ~ MonthlyIncome + cs_train[,15] + cs_train[,16] + cs_train[,17] + cs_train[,18] + cs_train[,19], family = binomial(link = "probit"), data = cs_train)
model4
```

```{r}
model5 <- glm(cs_train$SeriousDlqin2yrs ~ MonthlyIncome + cs_train[,15] + cs_train[,16] + cs_train[,17] + cs_train[,18] + cs_train[,19], family = binomial(link = "cloglog"), data = cs_train) #cloglog link
model5
```

```{r}
anova(model,model2,model3,model4,model5) #model 2 is the one with NA values
#model 1 is the best but we can check model 5 using our dummy variables
```

```{r}
x <- data.frame(cs_train$SeriousDlqin2yrs, cs_train$MonthlyIncome, cs_train[,15],cs_train[,16],cs_train[,17], cs_train[,18],cs_train[,19])
x <- na.omit(x) #Remember Monthly Income contains NA values
x_for_ridge <- data.matrix(x[,-1]) #Everything but Response Variable
ridge_model5 <- cv.glmnet(x_for_ridge,x[,1], alpha = 0, standardize = TRUE, nfolds = length(x))
ridge_model5
```

```{r}
lasso_model5 <- cv.glmnet(x_for_ridge,x[,1], alpha = 1, standardize = TRUE, nfolds = length(x))
lasso_model5
```


### Evaluate Final Model

- mmp plots to see if model fits the data, as well as the pearson Chi-square test
- checking for outliers and influential points
- Some measure of pesudo R-square and accuracy of the model
- Use confusion matrix, ROC/AUC curve, AIC to evaluate the different models



```{r}
# Split x and y variables
train.x = cs_train[,-which(names(cs_train) == "SeriousDlqin2yrs")]
train.y = cs_train$SeriousDlqin2yrs
test.x = cs_test[,-which(names(cs_test) == "SeriousDlqin2yrs")]
test.y = cs_test$SeriousDlqin2yrs
```

### Model after recoding Random Forest

```{r}
# Fits Random Forest
model.rf = randomForest(y = as.factor(train.y), x=train.x, xtest=test.x, ytest=as.factor(test.y), mtry = 3, importance = TRUE, na.action = na.omit)

model.rf # Output shows confusion matrix for both train and test

# Random Forest Output
var.imp = data.frame(importance(model.rf, type=2))
var.imp$Variables = row.names(var.imp)
varimp = var.imp[order(var.imp$MeanDecreaseGini,decreasing = T),]
par(mar = c(7.5,3,2,2)) 
giniplot = barplot(t(varimp[-2]/sum(varimp[-2])),las=2,cex.names=1, main="Gini Impurity Index Plot")
```


### Evaluation on Final Model using Training Data


```{r}
## Set model as final model
model.final <- model

## Evaluation on Final model using Training Data
train_preds = predict(model.final, newdata=train.x, type="response")
head(train_preds[is.na(train_preds)])

#train.x[c(7,9),]
#train_preds[is.na(train_preds)]

pred_compare = prediction(train_preds, train.y)
plot(performance(pred_compare, "acc"))
table(train.y, train_preds>0.2) # accurary on train
```



## Evaluating Model/Comparing results, Plots, Tests, chi-square test, influential points
# Need to Update!!

```{r}
# residual plots
#residualPlots(model.final)

# Marginial Model Plots
library(car)
mmp(model.final)
```
####  Goodness of Fit using Hosmer-Lemeshow Test

The p-value is 0, meaning that we want to reject the null hypothesis that the model is adequate.

```{r}
# Goodness of Fit using Hosmer-Lemeshow Test
linpred=predict(model.final)

cs_train_m <- mutate(cs_train, predprob=predict(model.final, type="response"))  # cal p^_i
gdf <- group_by(cs_train_m, ntile(linpred, 1000)) # group up the data by eta_x into 100 groups
hldf <- summarise(gdf, y=sum(SeriousDlqin2yrs==1), ppred=mean(predprob), count=n())
head(hldf)


# We adjust the size of the bins until there's only one group with less than 5
hldf[hldf$count<5,]

# Observed Proportion Confidence Interval vs Predicted Probability
hldf <- mutate(hldf, se.fit=sqrt(ppred*(1-ppred)/count))

ggplot(hldf,aes(x=ppred,y=y/count,ymin=y/count-2*se.fit, ymax=y/count+2*se.fit))+
  geom_point()+geom_linerange(color=grey(0.75))+
  geom_abline(intercept = 0,slope = 1)+
  xlab("Predicted Probability")+
  ylab("Observed Proportion")

# Hosmer-Lemeshow statistics
hlstat <- with(hldf, sum((y-count*ppred) ^2/(count * ppred * (1-ppred))))
c(hlstat, nrow(hldf))

# The p-value is given by:
1-pchisq(hlstat, nrow(hldf)-2)

```
### AUC
## Need to fix the Prediction function!!!
Predict -> Currently gives 120k, expecting 80k

```{r}

#Final Model(w/ interaction term ReugularMedicine * PhysicallyActive)
result_m2 = predict(model.final, newdata=test.x, type="response") 

# Fix Later!!! Forced the length to be the same
result_m2 <- result_m2[1:length(test.y)]
head(test.y)
pred_m2 = prediction(result_m2, test.y)


plot(performance(pred_m2, "acc")) #It seems like 0.52 cutoff has the highest accuracy
table(test.y, result_m2>0.2)


#Accuracy :
#Sensitivity : 
#Specificity : 
#The Specificity and accuracy improved a bit compared to the previous model without interaction term, sensitivity decreased a bit but still at a very high level.
plot(performance(pred_m2,"tpr","fpr"), colorize=T)
abline(0,1)

#Now we calculate the area under the curve (AUC) and accuracy of the model given above (glmModel2)
auc_ROCR2 <- performance(pred_m2, measure = "auc")
auc_ROCR2@y.values[[1]]
```

### Not Sure if we use this!!

```{r}

# Check how the model fits the data
# From model.final, we can calculate the difference in the two deviances from the summary(model): 987.27-513.52=473.75
#Number of regressors in the model: 813-802=11
pchisq(473.75,12)

#The area below 473.75 is one which means the area above it is almost zero. This means that our model has less error than intercept only model and explains some of the variance in the outcome variable.
print(paste("Pearson's X^2 =",round(sum(residuals(model.final,type="pearson")^2),3)))


qchisq(0.95,802)
#781.61<868.99, so we fail to reject the null hypothesis and conclude that the logistic model fits the data.
```
